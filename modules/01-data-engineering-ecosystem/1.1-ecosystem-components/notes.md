# Module 1.1: Data Engineering Ecosystem Components - Detailed Notes

## üìö Table of Contents
1. [Data Classification by Structure](#data-classification-by-structure)
2. [File Formats](#file-formats)
3. [Data Sources](#data-sources)
4. [Programming Languages](#programming-languages)
5. [Data Repositories](#data-repositories)
6. [ETL, ELT, and Data Pipelines](#etl-elt-and-data-pipelines)
7. [Big Data Fundamentals](#big-data-fundamentals)

---

## üóÇÔ∏è Data Classification by Structure

### Three Categories of Data

#### 1. Structured Data
**Definition**: Data that follows a rigid format and can be organized neatly into rows and columns.

**Characteristics**:
- Well-defined structure
- Adheres to a specified data model
- Stored in well-defined schemas
- Represented in tabular format (rows and columns)
- Objective facts and numbers
- Can be easily searched and analyzed

**Sources**:
- SQL Databases and RDBMS
- Online Transaction Processing (OLTP) systems
- Spreadsheets (Excel, Google Sheets)
- Online forms
- Sensors (GPS, RFID tags)
- Network and web server logs

**Storage**:
- Relational databases (SQL databases)
- Easy to examine with standard data analysis tools

**Example**:
```
Customer Table:
| CustomerID | Name      | Email              | City        |
|------------|-----------|-------------------|-------------|
| 001        | John Doe  | john@email.com    | New York    |
| 002        | Jane Smith| jane@email.com    | Los Angeles |
```

---

#### 2. Semi-Structured Data
**Definition**: Data that has some organizational properties but lacks a fixed or rigid schema.

**Characteristics**:
- Contains tags and elements (metadata)
- Uses metadata to organize in hierarchy
- Cannot be stored in rows and columns like databases
- Flexible structure
- Self-describing

**Sources**:
- E-mails
- XML documents
- JSON files
- Binary executables
- TCP/IP packets
- Zipped files
- Integration data from different sources

**Common Formats**:
- **XML**: Allows users to define tags and attributes
- **JSON**: Stores data in key-value pairs with hierarchy

**Storage**:
- NoSQL databases
- Document stores
- Files with hierarchical structure

**Example - JSON**:
```json
{
  "customer": {
    "id": "001",
    "name": "John Doe",
    "contact": {
      "email": "john@email.com",
      "phones": ["555-1234", "555-5678"]
    },
    "orders": [
      {"order_id": "A001", "amount": 99.99},
      {"order_id": "A002", "amount": 149.50}
    ]
  }
}
```

---

#### 3. Unstructured Data
**Definition**: Data that does not have an easily identifiable structure and cannot be organized in rows and columns.

**Characteristics**:
- No particular format, sequence, semantics, or rules
- Complex and mostly qualitative
- Cannot be reduced to rows and columns
- Heterogeneous sources
- Requires specialized processing

**Sources**:
- Web pages
- Social media feeds
- Images (JPEG, GIF, PNG)
- Video and audio files
- Documents and PDFs
- PowerPoint presentations
- Media logs
- Surveys
- Text files

**Storage**:
- Files and documents (for manual analysis)
- NoSQL databases (with specialized analysis tools)
- Data lakes

**Business Applications**:
- Business intelligence
- Analytics
- Sentiment analysis
- Image recognition
- Natural language processing

**Note**: Approximately **80% of data** is considered unstructured.

---

## üìÑ File Formats

### 1. Delimited Text Files

#### CSV (Comma-Separated Values)
**Structure**: Values separated by commas

**Characteristics**:
- Most common delimited format
- First row = column headers
- Each row = one record
- Each column can have different data types (date, string, integer)

**Example**:
```csv
Name,Age,City,Salary
John Doe,35,New York,75000
Jane Smith,28,Los Angeles,65000
```

**Advantages**:
- Straightforward information schema
- Standard format
- Processed by almost all applications
- Allows field values of any length

#### TSV (Tab-Separated Values)
**Structure**: Values separated by tabs

**Use Case**: When commas appear in the actual data (e.g., addresses, descriptions)

**Advantages**:
- Tab stops are infrequent in running text
- Alternative when commas can't be used as delimiters

---

### 2. Microsoft Excel Open XML Spreadsheet (XLSX)

**Characteristics**:
- XML-based file format
- Multiple worksheets in one workbook
- Organized into rows and columns
- Data stored at cell intersections
- Open file format (accessible to most applications)

**Advantages**:
- Can use all Excel functions
- Secure (cannot save malicious code)
- Generally accessible to other applications
- Supports complex calculations and formulas

**Structure**:
```
Workbook (file.xlsx)
‚îú‚îÄ‚îÄ Worksheet 1
‚îÇ   ‚îú‚îÄ‚îÄ Row 1: Headers
‚îÇ   ‚îú‚îÄ‚îÄ Row 2: Data
‚îÇ   ‚îî‚îÄ‚îÄ Row n: Data
‚îú‚îÄ‚îÄ Worksheet 2
‚îî‚îÄ‚îÄ Worksheet n
```

---

### 3. XML (Extensible Markup Language)

**Characteristics**:
- Markup language with set rules for encoding data
- Human and machine-readable
- Self-descriptive language
- Platform independent
- Programming language independent

**Use Cases**:
- Sending information over the internet
- Online surveys
- Bank statements
- Unstructured datasets

**Example**:
```xml
<?xml version="1.0" encoding="UTF-8"?>
<customer>
    <id>001</id>
    <name>John Doe</name>
    <email>john@email.com</email>
    <orders>
        <order>
            <order_id>A001</order_id>
            <amount>99.99</amount>
        </order>
    </orders>
</customer>
```

**Advantages**:
- Does not use predefined tags (unlike HTML)
- Simplifies data sharing between systems
- Independent of platform and language

---

### 4. PDF (Portable Document Format)

**Characteristics**:
- Developed by Adobe
- Independent of application software, hardware, and OS
- Viewed the same way on any device

**Use Cases**:
- Legal documents
- Financial documents
- Forms that need to be filled in
- Documents requiring consistent formatting

**Advantages**:
- Platform-independent viewing
- Preserves formatting
- Can include interactive forms

---

### 5. JSON (JavaScript Object Notation)

**Characteristics**:
- Text-based open standard
- Language-independent data format
- Can be read in any programming language
- Compatible with wide range of browsers

**Use Cases**:
- APIs and web services (most return JSON)
- Transmitting structured data over the web
- Sharing data of any size and type (even audio/video)

**Example**:
```json
{
  "transaction": {
    "id": "TXN001",
    "customer_id": "CUST123",
    "items": [
      {"product": "Laptop", "price": 999.99},
      {"product": "Mouse", "price": 29.99}
    ],
    "total": 1029.98,
    "timestamp": "2025-02-15T10:30:00Z"
  }
}
```

**Advantages**:
- Easy to use
- Compatible with browsers
- Best tool for sharing data
- Widely used in APIs

---

## üåê Data Sources

### 1. Relational Databases (Internal Sources)

**Purpose**: Support day-to-day business activities

**Common RDBMS**:
- SQL Server
- Oracle
- MySQL
- IBM DB2
- PostgreSQL

**Use Cases**:
- Customer transactions
- Human resource activities
- Workflows
- Retail transaction analysis
- CRM data for sales projections

**Example**:
```
Retail Transaction System ‚Üí Sales Analysis by Region
CRM System ‚Üí Sales Projections
HR System ‚Üí Employee Analytics
```

---

### 2. External Datasets

#### Government Data
- Demographic data
- Economic datasets
- Released on ongoing basis
- Publicly available

#### Commercial Data
- Point-of-Sale data
- Financial data
- Weather data
- Market research data

**Formats**:
- Flat files
- Spreadsheet files
- XML documents

---

### 3. Flat Files and Spreadsheets

#### Flat Files
**Characteristics**:
- Store data in plain text
- One record per line
- Values separated by delimiters (commas, semicolons, tabs)
- Maps to a single table

**Most Common**: CSV format

#### Spreadsheets
**Characteristics**:
- Special type of flat file
- Tabular format (rows and columns)
- Can contain multiple worksheets
- Each worksheet maps to different table
- Can include formatting and formulas

**Common Formats**:
- Microsoft Excel (.XLS, .XLSX)
- Google Sheets
- Apple Numbers
- LibreOffice

---

### 4. XML Files

**Characteristics**:
- Data values identified by tags
- Can support complex hierarchical structures
- More complex than flat files

**Use Cases**:
- Online surveys
- Bank statements
- Unstructured datasets

---

### 5. APIs and Web Services

**Definition**: Application Program Interfaces that allow applications to interact and obtain data

**How They Work**:
1. Listen for incoming requests (web or network)
2. Return data in various formats (plain text, XML, HTML, JSON, media files)

**Popular Examples**:

#### Social Media APIs
- **Twitter API**: Opinion mining, sentiment analysis
- **Facebook API**: Posts analysis, user behavior

**Use Case**: Summarize appreciation/criticism on topics (government policies, products, services)

#### Financial APIs
- **Stock Market APIs**: Share prices, commodity prices, earnings per share, historical data
- **Use Case**: Trading and financial analysis

#### Validation APIs
- **Data Lookup APIs**: Clean and prepare data
- **Example**: Check which city a zip code belongs to

#### Database APIs
- Pull data from internal and external databases
- Integration with organizational data sources

---

### 6. Web Scraping

**Also Known As**: Screen scraping, web harvesting, web data extraction

**Definition**: Extracting relevant data from unstructured web sources

**Capabilities**:
- Extract text
- Collect contact information
- Download images and videos
- Gather product items
- Much more from websites

**Use Cases**:
1. **Price Comparison**: Collect product details from retailers and eCommerce sites
2. **Lead Generation**: Extract data from public sources
3. **Forum Data**: Collect posts and author information
4. **ML Training**: Gather datasets for machine learning models

**Popular Tools**:
- BeautifulSoup
- Scrapy
- Pandas
- Selenium

---

### 7. Data Streams

**Definition**: Constant streams of data from instruments, devices, and applications

**Characteristics**:
- Generally timestamped
- Geo-tagged for geographical identification
- Continuous flow
- Real-time processing

**Sources and Use Cases**:
1. **Financial**: Stock and market tickers for trading
2. **Retail**: Transaction streams for demand prediction and supply chain
3. **Security**: Surveillance and video feeds for threat detection
4. **Social Media**: Feeds for sentiment analysis
5. **Industrial**: Sensor data for monitoring machinery
6. **Web Analytics**: Click feeds for performance monitoring
7. **Aviation**: Real-time flight events for rebooking

**Popular Processing Tools**:
- Apache Kafka
- Apache Spark Streaming
- Apache Storm

---

### 8. RSS Feeds

**Definition**: Really Simple Syndication - captures updated data from online sources

**Use Cases**:
- Online forums
- News sites
- Blogs
- Any source with ongoing updates

**How It Works**:
1. RSS text files contain updates
2. Feed reader converts RSS to stream
3. Updates delivered to user devices

**Benefits**:
- Real-time data updates
- Standardized format
- Easy integration

---

## üíª Programming Languages for Data Professionals

### 1. SQL (Structured Query Language)

**Category**: Query Language

**Purpose**: Accessing and manipulating data in databases (primarily relational)

**Capabilities**:
- Insert, update, delete records
- Create databases, tables, views
- Write stored procedures

**Syntax Examples**:
```sql
-- Insert data
INSERT INTO customers (name, email) VALUES ('John Doe', 'john@email.com');

-- Update data
UPDATE customers SET city = 'New York' WHERE customer_id = 123;

-- Query data
SELECT * FROM customers WHERE city = 'New York';

-- Create table
CREATE TABLE orders (
    order_id INT PRIMARY KEY,
    customer_id INT,
    order_date DATE
);
```

**Advantages**:
- Portable and platform-independent
- Works with wide variety of databases and data repositories
- Simple syntax similar to English
- Retrieves large amounts of data quickly
- Runs on interpreter system (quick prototyping)
- Large user community and documentation

**Keywords**: SELECT, INSERT, UPDATE, DELETE, CREATE, DROP

---

### 2. Python

**Category**: Programming Language

**Type**: Open-source, general-purpose, high-level

**Characteristics**:
- Fewer lines of code compared to older languages
- Easy to learn
- Low learning curve
- Large developer community
- Focus on simplicity and readability

**Why Python for Data Engineering**:
1. **High-computational tasks**: Efficient with vast amounts of data
2. **Parallel processing**: Libraries like NumPy and Pandas
3. **Built-in functions**: For frequently used concepts
4. **Multiple paradigms**: Object-oriented, imperative, functional, procedural

**Key Libraries for Data Engineering**:

**Data Manipulation & Analysis**:
- **Pandas**: Data cleaning and analysis
- **NumPy**: Numerical operations
- **SciPy**: Statistical analysis

**Web Scraping**:
- **BeautifulSoup**: Parse HTML/XML
- **Scrapy**: Web crawling framework

**Data Visualization**:
- **Matplotlib**: Charts and graphs
- **Seaborn**: Statistical visualizations

**Image Processing**:
- **OpenCV**: Computer vision

**Example**:
```python
import pandas as pd
import numpy as np

# Read data
df = pd.read_csv('sales_data.csv')

# Clean data
df.dropna(inplace=True)

# Transform data
df['revenue'] = df['quantity'] * df['price']

# Aggregate
summary = df.groupby('region')['revenue'].sum()
```

**Advantages**:
- Free and open-source
- Cross-platform (Windows, Linux)
- Community-based development
- Vast array of libraries
- Widespread support

---

### 3. R

**Category**: Programming Language

**Type**: Open-source for data analysis, visualization, ML, and statistics

**Strengths**:
- Creating compelling visualizations
- Statistical software development
- Data analytics

**Key Benefits**:
- Platform-independent
- Can pair with Python and other languages
- Highly extensible (add new functions)
- Handles structured and unstructured data
- Comprehensive data capability

**Popular Libraries**:
- **Ggplot2**: Aesthetic graphical plots
- **Plotly**: Interactive visualizations

**Capabilities**:
- Generate reports with embedded data and scripts
- Create interactive web apps
- Dominant for developing statistical tools

**Use Cases**:
- Statistical analysis
- Data visualization
- Academic research
- Bioinformatics

---

### 4. Java

**Category**: Programming Language

**Type**: Object-oriented, class-based, platform-independent

**Data Engineering Uses**:
- Cleaning data
- Importing and exporting data
- Statistical analysis
- Data visualization

**Big Data Ecosystem**:
Most popular frameworks written in Java:
- **Hadoop**: Distributed storage and processing
- **Hive**: Data warehousing on Hadoop
- **Spark**: Distributed analytics

**Why Java for Data Engineering**:
- Speed-critical projects
- Scalability
- Platform independence
- Robust ecosystem

---

### 5. Shell Scripting (Unix/Linux Shell)

**Category**: Scripting Language

**Definition**: Series of UNIX commands in plain text file

**Characteristics**:
- Fast and easy to write
- Ideal for repetitive tasks
- Command-line automation

**Typical Operations**:
- File manipulation
- Program execution
- System administration (disk backups, system logs)
- Installation scripts
- Routine backups
- Running batches

**Example**:
```bash
#!/bin/bash
# Backup database
pg_dump mydb > /backups/mydb_$(date +%Y%m%d).sql

# Clean old logs
find /logs -name "*.log" -mtime +30 -delete
```

**When to Use**:
- Automating repetitive tasks
- System administration
- Batch processing
- ETL operations

---

### 6. PowerShell

**Category**: Automation Tool & Scripting Language

**Creator**: Microsoft

**Type**: Cross-platform

**Optimized For**:
- Structured data formats (JSON, CSV, XML)
- REST APIs
- Websites
- Office applications

**Components**:
- Command-line shell
- Scripting language

**Features**:
- Object-based (not text-based)
- Filter, sort, measure, group, compare objects
- Data pipeline operations

**Use Cases**:
- Data mining
- Building GUIs
- Creating charts and dashboards
- Interactive reports
- Windows system administration

---

## üóÑÔ∏è Data Repositories

### Overview

**Definition**: General term for data that has been collected, organized, and isolated for business operations, reporting, and analysis

**Can Be**: Small or large database infrastructure with one or more databases

**Types**:
1. Databases (Transactional and Analytical)
2. Data Warehouses
3. Data Marts
4. Data Lakes
5. Big Data Stores

---

### Databases

**Definition**: Collection of data designed for input, storage, search, retrieval, and modification

**DBMS (Database Management System)**:
- Set of programs that creates and maintains databases
- Allows storing, modifying, and extracting information
- Query function for data retrieval

**Selection Factors**:
- Data type and structure
- Querying mechanisms
- Latency requirements
- Transaction speeds
- Intended use of data

---

### Two Main Database Types

#### 1. Relational Databases (RDBMS)

**Structure**:
- Tabular format (rows and columns)
- Well-defined structure and schema
- Optimized for data operations and querying

**Query Language**: SQL (Structured Query Language)

**Characteristics**:
- Tables can be linked based on common data
- Rows = records, Columns = attributes
- Relationships minimize data redundancy
- Field restrictions for data types/values
- Greater consistency and data integrity

**Popular RDBMS**:
- IBM DB2
- Microsoft SQL Server
- MySQL
- Oracle Database
- PostgreSQL

**Cloud-based RDBMS** (Database-as-a-Service):
- Amazon RDS
- Google Cloud SQL
- IBM DB2 on Cloud
- Oracle Cloud
- SQL Azure

**Advantages**:
1. **Flexibility**: Add columns/tables while database is running
2. **Reduced Redundancy**: Data minimized through normalization
3. **Backup & Recovery**: Easy export/import, continuous mirroring
4. **ACID Compliance**: Atomicity, Consistency, Isolation, Durability
5. **Mature Technology**: Well-documented, easy to find talent
6. **Joining Tables**: Create meaningful information

**Use Cases**:

**OLTP (Online Transaction Processing)**:
- High transaction rates
- Large number of users
- Frequent queries and updates
- Fast response times
- Insert, update, delete small amounts

**Data Warehouses**:
- OLAP (Online Analytical Processing)
- Historical data analysis
- Business intelligence

**IoT Solutions**:
- Speed requirements
- Collect from edge devices
- Lightweight database solution

**Limitations**:
1. Does not work well with semi-structured/unstructured data
2. Schema must be identical for migration between RDBMSs
3. Limit on data field lengths
4. Not suitable for extensive analytics on unstructured data

---

#### 2. NoSQL Databases (Non-Relational)

**Definition**: "Not Only SQL" - non-relational database with flexible schemas

**History**: Existed for years but became popular with cloud, big data, IoT, and social media

**Key Attributes**:
- Scale
- Performance
- Ease of use

**Characteristics**:
- Schema-less or free-form fashion
- Built for specific data models
- Flexible schemas
- Do not use traditional row/column/table design
- Typically don't use SQL (though some support SQL-like interfaces)

**Four Types of NoSQL Databases**:

##### A. Key-Value Store

**Structure**: Data stored as key-value pairs

**Characteristics**:
- Key = unique identifier
- Value = can be anything (simple or complex JSON)

**Use Cases**:
- User session data
- User preferences
- Real-time recommendations
- Targeted advertising
- In-memory data caching

**Not Good For**:
- Query on specific data values
- Relationships between data values
- Multiple unique keys

**Examples**: Redis, Memcached, DynamoDB

##### B. Document-Based

**Structure**: Each record stored with data in single document

**Characteristics**:
- Flexible indexing
- Powerful ad hoc queries
- Analytics over collections

**Use Cases**:
- eCommerce platforms
- Medical records storage
- CRM platforms
- Analytics platforms

**Not Good For**:
- Complex search queries
- Multi-operation transactions

**Examples**: MongoDB, DocumentDB, CouchDB, Cloudant

##### C. Column-Based

**Structure**: Data stored in cells grouped as columns (not rows)

**Concept**: Column Family = columns usually accessed together

**Example**:
```
Column Family: Customer_Profile
- name
- email
- address

Column Family: Customer_Transactions
- transaction_id
- amount
- date
```

**Characteristics**:
- All cells of column stored as continuous disk entry
- Very fast access and searching

**Use Cases**:
- Systems requiring heavy write requests
- Time-series data
- Weather data
- IoT data

**Not Good For**:
- Complex queries
- Frequently changing query patterns

**Examples**: Cassandra, HBase

##### D. Graph-Based

**Structure**: Graphical model with nodes and relationships

**Components**:
- Nodes (circles) = contain data
- Arrows = represent relationships

**Use Cases**:
- Social networks
- Real-time product recommendations
- Network diagrams
- Fraud detection
- Access management
- Connected data analysis

**Not Good For**:
- High-volume transactions
- Large-volume analytics queries

**Examples**: Neo4J, CosmosDB

---

**NoSQL Advantages**:
1. Handle large volumes of structured, semi-structured, unstructured data
2. Distributed systems across multiple data centers
3. Cloud computing infrastructure
4. Efficient cost-effective scale-out architecture
5. Simpler design
6. Better availability control
7. Improved scalability
8. Agile and flexible

**RDBMS vs NoSQL Summary**:

| Aspect | RDBMS | NoSQL |
|--------|-------|-------|
| **Schema** | Rigid, predefined | Schema-agnostic, flexible |
| **Data Types** | Structured only | Structured, semi-structured, unstructured |
| **Cost** | High-end, expensive | Low-cost commodity hardware |
| **ACID** | Yes | Mostly no |
| **Maturity** | Mature, well-documented | Relatively newer |
| **Scalability** | Vertical (scale up) | Horizontal (scale out) |

---

### Data Warehouses

**Definition**: Central repository integrating data from multiple sources

**Purpose**: Single source of truth for current and historical data

**Characteristics**:
- Data already modeled and structured
- Analysis-ready
- Cleansed, conformed, categorized
- Traditionally stores relational data
- Now includes non-relational with NoSQL technologies

**Three-Tier Architecture**:

**Bottom Tier**: Database Servers
- Relational, non-relational, or both
- Extract data from sources

**Middle Tier**: OLAP Server
- Process and analyze information
- From multiple database servers

**Top Tier**: Client Front-End
- Tools and applications
- Querying, reporting, analyzing

**Cloud Data Warehouses Benefits**:
- Lower costs
- Limitless storage and compute
- Scale on pay-as-you-go basis
- Faster disaster recovery

**When to Use**:
- Massive amounts of operational data
- Readily available for reporting and analysis

**Popular Data Warehouses**:
- Teradata Enterprise Data Warehouse
- Oracle Exadata
- IBM Db2 Warehouse on Cloud
- IBM Netezza Performance Server
- Amazon RedShift
- Google BigQuery
- Cloudera Enterprise Data Hub
- Snowflake Cloud Data Warehouse

---

### Data Marts

**Definition**: Sub-section of data warehouse for specific business function or user group

**Examples**:
- Sales team data mart
- Finance team data mart

**Three Types**:

#### 1. Dependent Data Marts
- Sub-section of enterprise data warehouse
- Data already cleaned and transformed
- Isolated security and performance

#### 2. Independent Data Marts
- Created from other sources (not DWH)
- Internal operational systems
- External data
- Need to transform source data

#### 3. Hybrid Data Marts
- Combine inputs from:
  - Data warehouses
  - Operational systems
  - External systems

**Purpose**:
- Provide relevant data to specific users
- Accelerate business processes
- Efficient response times
- Cost and time-efficient decisions
- Improve end-user response time
- Secure access and control

---

### Data Lakes

**Definition**: Repository storing large amounts of data in native format

**Characteristics**:
- Structured, semi-structured, unstructured data
- No need to define structure before loading
- Don't need to know all use cases upfront
- Raw data in native format
- Transformed based on use case

**Not a Data Swamp**:
- Appropriately classified
- Protected
- Governed

**Reference Architecture**: Independent of technology

**Technologies**:
- Cloud Object Storage (Amazon S3)
- Large-scale distributed systems (Apache Hadoop)
- Relational database management systems
- NoSQL repositories

**Benefits**:
1. **Store All Types**: Unstructured, semi-structured, structured
2. **Agile Scaling**: Terabytes to petabytes
3. **Save Time**: No upfront structure definition
4. **Repurpose Data**: Wide-ranging use cases
5. **Future-Proof**: Hard to foresee all future uses

**Popular Vendors**:
- Amazon
- Cloudera
- Google
- IBM
- Informatica
- Microsoft
- Oracle
- SAS
- Snowflake
- Teradata
- Zaloni

---

### Data Lakehouse (Modern Approach)

**Definition**: Combines best of data lakes and data warehouses

**Restaurant Analogy**:
- Loading dock = Data Lake (raw ingredients)
- Pantry/Freezer = Data Warehouse (organized, ready to use)
- Lakehouse = Efficient system combining both

**Data Lake Challenges**:
- Data governance issues
- Data quality problems
- Can become "data swamps"
- Duplicate, inaccurate, incomplete data
- Stale data loses value
- Query performance issues

**Data Warehouse Challenges**:
- High cost (can't store everything)
- Limited support for semi-structured/unstructured data
- Slow for applications requiring fresh data
- Time to sort, clean, and load

**Lakehouse Benefits**:
- Flexibility and cost-effectiveness of data lake
- Performance and structure of data warehouse
- Built-in data management and governance
- Supports both BI and ML workloads
- Low-cost storage for diverse sources

**Use Cases**:
- Modernize existing data lakes
- Complement data warehouses
- Support AI/ML-driven workloads
- New types of analytics

---

## üîÑ ETL, ELT, and Data Pipelines

### ETL (Extract, Transform, Load)

**Definition**: Automated process converting raw data into analysis-ready data

**Purpose**: Core process for gaining value from data

**Three Steps**:

#### 1. Extract
**Goal**: Gather raw data from identified sources

**Methods**:

**Batch Processing**:
- Move data in large chunks
- Scheduled intervals
- **Tools**: Stitch, Blendo

**Stream Processing**:
- Pull data in real-time
- Transform while in transit
- Before loading to repository
- **Tools**: Apache Samza, Apache Storm, Apache Kafka

#### 2. Transform
**Goal**: Execute rules and functions to convert raw data

**Operations**:
- Make date formats consistent
- Standardize units of measurement
- Remove duplicate data
- Filter unnecessary data
- Enrich data (e.g., split full name)
- Establish key relationships across tables
- Apply business rules
- Data validations

**Example Transformations**:
```
Raw: "John Doe", "01/15/2025", "$1,234.56"
Transformed: 
  - First Name: "John"
  - Last Name: "Doe"
  - Date: "2025-01-15"
  - Amount: 1234.56
```

#### 3. Load
**Goal**: Transport processed data to destination

**Types**:

**Initial Loading**:
- Populate all data in repository
- First-time setup

**Incremental Loading**:
- Ongoing updates and modifications
- Periodic changes

**Full Refresh**:
- Erase contents and reload
- Fresh data

**Load Verification** (Critical):
- Check for missing/null values
- Monitor server performance
- Track load failures
- Recovery mechanisms

**Traditional Use**: Batch workloads on large scale

**Modern Use**: Real-time streaming with streaming ETL tools

**Popular ETL Tools**:
- IBM InfoSphere Information Server
- AWS Glue
- Improvado
- Skyvia
- HEVO
- Informatica PowerCenter

---

### ELT (Extract, Load, Transform)

**Definition**: Variation where data loaded first, then transformed in target system

**Key Difference**: Transformations applied in destination system

**Typical Destination**:
- Data lake (most likely)
- Data warehouse (also possible)

**Technology**: Relatively new, powered by cloud

**Why ELT**:
- Process large sets of unstructured/non-relational data
- Ideal for data lakes
- Transformations applied after raw data loaded

**Advantages**:
1. **Shorter Cycle**: Data delivered directly to destination (no staging)
2. **Immediate Ingestion**: Raw data available as soon as it arrives
3. **Greater Flexibility**: For exploratory data analytics
4. **Selective Transformation**: Only transform what's needed
5. **Multiple Use Cases**: Same raw data for different purposes
6. **Big Data Friendly**: More suited for Big Data
7. **No Restructuring**: Don't need to modify entire warehouse for new use case

**ELT vs ETL Comparison**:

| Aspect | ETL | ELT |
|--------|-----|-----|
| **Transform Location** | Before loading | After loading |
| **Best For** | Structured data | All data types |
| **Destination** | Data warehouse | Data lake |
| **Speed** | Slower (transformation upfront) | Faster (load immediately) |
| **Flexibility** | Less flexible | More flexible |
| **Use Case** | Traditional BI | Modern analytics, ML |

---

### Data Pipelines

**Definition**: Broader term encompassing entire journey of data from source to destination

**Relationship**: ETL and ELT are subsets of data pipelines

**Characteristics**:
- Can be batch processing
- Can be streaming data
- Can be combination of both

**Streaming Data Processing**:
- Continuous flow transformation
- Useful for data needing constant updating
- **Example**: Sensor monitoring traffic

**Capabilities**:
- High-performing system
- Supports long-running batch queries
- Supports smaller interactive queries

**Typical Destination**: Data lake (though can be other targets)

**Other Destinations**:
- Applications
- Visualization tools
- Data warehouses
- Analytics platforms

**Popular Data Pipeline Solutions**:
- Apache Beam
- Apache Airflow
- Google DataFlow

**Pipeline vs ETL/ELT**:
```
Data Pipeline (Broad Term)
    ‚îú‚îÄ‚îÄ ETL Process
    ‚îú‚îÄ‚îÄ ELT Process
    ‚îú‚îÄ‚îÄ Streaming Pipelines
    ‚îú‚îÄ‚îÄ Batch Pipelines
    ‚îî‚îÄ‚îÄ Hybrid Pipelines
```

---

## üìä Big Data Fundamentals

### What is Big Data?

**Ernst & Young Definition**:
> "Big Data refers to the dynamic, large and disparate volumes of data being created by people, tools, and machines. It requires new, innovative, and scalable technology to collect, host, and analytically process the vast amount of data gathered in order to derive real-time business insights."

**Key Point**: No single definition, but common elements

---

### The 5 V's of Big Data

#### 1. Velocity
**Definition**: Speed at which data accumulates

**Characteristics**:
- Data generated extremely fast
- Process never stops
- Near or real-time streaming
- Local and cloud-based processing

**Example**: Every 60 seconds, hours of footage uploaded to YouTube

**Technologies**: Real-time streaming platforms

---

#### 2. Volume
**Definition**: Scale of data or increase in amount stored

**Characteristics**:
- Massive amounts of data
- Continuous growth
- Scalable infrastructure needed

**Drivers**:
- Increase in data sources
- Higher resolution sensors
- Scalable infrastructure

**Example**: 
- 7 billion people using digital devices
- ~2.5 quintillion bytes generated daily
- Equivalent to 10 million Blu-ray DVDs

---

#### 3. Variety
**Definition**: Diversity of data types and sources

**Data Types**:
- **Structured**: Rows and columns (relational databases)
- **Unstructured**: Not pre-defined (tweets, blog posts, pictures, video)
- **Semi-structured**: Mix of both

**Sources**:
- Machines
- People
- Processes
- Internal systems
- External systems

**Drivers**:
- Mobile technologies
- Social media
- Wearable technologies
- Geo technologies
- Video
- IoT devices

**Example**: Text, pictures, film, sound, health data from wearables, IoT sensor data

---

#### 4. Veracity
**Definition**: Quality and origin of data, conformity to facts and accuracy

**Attributes**:
- Consistency
- Completeness
- Integrity
- Ambiguity

**Challenges**:
- 80% of data is unstructured
- Must produce reliable insights
- Accuracy in digital era
- Real vs. false information

**Drivers**:
- Cost considerations
- Need for traceability

**Requirements**:
- Data must be categorized
- Data must be analyzed
- Data must be visualized

---

#### 5. Value
**Definition**: Ability to turn data into value

**Types of Value**:
- Profit (business value)
- Medical benefits
- Social benefits
- Customer satisfaction
- Employee satisfaction
- Personal satisfaction

**Key Point**: Main reason to invest in Big Data - derive value

---

### Big Data Processing Tools

**Challenge**: Conventional data analysis tools not feasible for Big Data scale

**Solution**: Tools leveraging distributed computing power

**Key Technologies**:

#### 1. Apache Hadoop
**Type**: Collection of tools for distributed storage and processing

**Language**: Java-based, open-source framework

**Capabilities**:
- Distributed storage
- Processing of large datasets
- Across clusters of computers
- Scale from single node to thousands

**Architecture**:
- **Node**: Single computer
- **Cluster**: Collection of nodes

**Benefits**:
- Reliable
- Scalable
- Cost-effective
- No format requirements

**Use Cases**:
- Incorporate emerging data formats (streaming audio, video, social media, clickstreams)
- Real-time, self-service access
- Optimize enterprise data warehouse costs
- Move "cold" data (infrequent use) to Hadoop

---

#### 2. HDFS (Hadoop Distributed File System)

**Purpose**: Storage system for Big Data

**Characteristics**:
- Runs on multiple commodity hardware
- Connected through network
- Scalable and reliable

**How It Works**:
1. Splits large files across multiple computers
2. Allows parallel access
3. Replicates file blocks on different nodes
4. Prevents data loss (fault-tolerant)

**Example - Phone Book**:
```
Phone Book (All US numbers)
‚îú‚îÄ‚îÄ Server 1: Last names A-D
‚îú‚îÄ‚îÄ Server 2: Last names E-H
‚îú‚îÄ‚îÄ Server 3: Last names I-L
‚îî‚îÄ‚îÄ Server 4: Last names M-P
    ‚îî‚îÄ‚îÄ Each replicated 2x on other servers
```

**Benefits**:
1. **Higher Availability**: Multiple copies
2. **Scalability**: Break work into smaller chunks
3. **Data Locality**: Move computation to data (not data to computation)
   - Minimizes network congestion
   - Increases throughput
4. **Fast Recovery**: Built-in fault detection
5. **Streaming Data**: Supports high throughput rates
6. **Large Data Sets**: Scales to hundreds of nodes
7. **Portability**: Multiple hardware platforms and OS

---

#### 3. Apache Hive

**Type**: Open-source data warehouse software

**Built On**: Hadoop

**Purpose**: Read, write, manage large datasets in HDFS or other storage (HBase)

**Characteristics**:
- Intended for long sequential scans
- Very high latency queries
- Not suitable for fast response times
- Read-based (not for transaction processing)

**Best For**:
- Data warehousing tasks
- ETL operations
- Reporting
- Data analysis

**Interface**: SQL-like language for easy data access

**Not Good For**:
- Applications needing fast response
- High percentage of write operations
- Transaction processing (OLTP)

---

#### 4. Apache Spark

**Type**: General-purpose data processing engine

**Purpose**: Extract and process large volumes of data

**Key Feature**: In-memory processing
- Significantly increases speed
- Spills to disk only when memory constrained

**Applications**:
- Interactive Analytics
- Stream Processing
- Machine Learning
- Data Integration
- ETL operations

**Supported Languages**:
- Java
- Scala
- Python
- R
- SQL

**Deployment Options**:
- Standalone clustering
- On top of Hadoop
- Other infrastructures

**Data Sources**:
- HDFS
- Hive
- Many others (highly versatile)

**Key Use Case**: 
- Process streaming data fast
- Perform complex analytics in real-time
- Low-latency processing

---

### Big Data Analytics Workflow

```
Data Sources (IoT, Social Media, Transactions)
    ‚Üì
Ingestion (Kafka, Flume)
    ‚Üì
Storage (HDFS, Cloud Storage)
    ‚Üì
Processing (Spark, Hadoop MapReduce)
    ‚Üì
Analysis (Hive, Spark SQL)
    ‚Üì
Visualization (Tableau, Power BI)
    ‚Üì
Insights & Actions
```

---

## üéØ Key Takeaways

### Data Structure
1. **80% of data is unstructured** - requires specialized tools
2. **Semi-structured data** (JSON, XML) bridges structured and unstructured
3. **Choose storage** based on data structure and use case

### File Formats
1. **CSV/TSV** - Simple, universal, but limited functionality
2. **JSON** - Best for APIs and web services
3. **XML** - Good for hierarchical data and data exchange
4. **XLSX** - Rich functionality but not ideal for automation

### Data Sources
1. **APIs are king** - Most modern data comes through APIs
2. **Streaming is growing** - Real-time data increasingly important
3. **Web scraping** - Powerful but use responsibly

### Languages
1. **SQL is non-negotiable** - Every data professional needs it
2. **Python is the Swiss Army knife** - Most versatile for data engineering
3. **Shell scripting** - Essential for automation
4. **R for statistics** - Best for advanced analytics

### Repositories
1. **RDBMS for structure** - When data fits rows/columns
2. **NoSQL for flexibility** - When schema changes frequently
3. **Data lakes for everything** - Raw data storage
4. **Data warehouses for analytics** - Processed, structured data
5. **Lakehouses for modern needs** - Best of both worlds

### ETL/ELT
1. **ETL is traditional** - Transform before load
2. **ELT is modern** - Load then transform (cloud-powered)
3. **Data pipelines are broader** - Encompass entire data journey
4. **Choose based on use case** - Not one-size-fits-all

### Big Data
1. **5 V's framework** - Velocity, Volume, Variety, Veracity, Value
2. **Hadoop ecosystem** - Foundation for Big Data
3. **Spark for speed** - In-memory processing wins
4. **Distributed computing** - Only way to handle scale

---

## üí° Practical Tips

### For Daily Work
1. **Understand your data** - Structure determines tools
2. **Start with SQL** - Query everything
3. **Automate with Python** - Script repetitive tasks
4. **Use version control** - Git for all code
5. **Monitor pipelines** - Know when things break

### For Learning
1. **Practice with real data** - Theory + hands-on
2. **Build end-to-end pipelines** - Source to destination
3. **Learn one language deeply** - Then expand
4. **Understand trade-offs** - Every tool has pros/cons

### For Interviews
1. **Know the 5 V's** - Classic Big Data question
2. **Explain ETL vs ELT** - Common interview topic
3. **Compare RDBMS vs NoSQL** - Understand use cases
4. **Describe your tech stack** - What you've actually used

---

**Last Updated**: February 15, 2025
**Module Status**: Complete
**Next**: Module 2 - Python for Data Engineering